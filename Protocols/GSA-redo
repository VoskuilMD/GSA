# Author: Michiel Voskuil 

# Date: 2017/11/07

1a. Copy scripts to appropriate locations
----
```
# Set run directory in which input file is stored
RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]

mkdir $RUNDIR/scripts
```

Make sure the following scripts are in $RUNDIR/scripts
GS_to_OptiCall.sh
OptiCall_to_plink.py
HRC-1000G-check-bim.pl

Make sure the following scripts are in $RUNDIR
set_opticall_variables.sh
create_opticall_jobs.sh


1.Load tools and set variables for SLURM environment / Calculon cluster UMCG
-------------------------------------------------------------------------

```
#!/bin/bash
#SBATCH --job-name=GSA.1
#SBATCH --output=GSA.1.out
#SBATCH --error=GSA.1.err
#SBATCH --time=40:00:00
#SBATCH --cpus-per-task=6
#SBATCH --mem=70gb
#SBATCH --nodes=1
#SBATCH --get-user-env=L

module load plink
module load opticall
module load Perl
module load Python
module load PerlPlus
module load Java
module load VCFtools

#####Set environment variables here

#Run directory in which input file is stored
RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]

#Name of input file [final report table from Illumina Genome Studio Software]
input=GSA.final_report.txt

#Final report: column with snp name
s=1
#Final report: column with allele, p.e. [A/G]
a=23
#Final report: column with snp position
c=20
#Final report: column with chromosome
x=19
#Final report: column with illumina norm. intensity for allele 1
A=30
#Final report: column with illumina norm. intensity for allele 2
B=31
```

2. Convert GS output into optiCall input files per chromosome
-------------------------------------------------------------------------

```
cd $RUNDIR
mkdir $RUNDIR/opticall_input
bash scripts/GS_to_OptiCall.sh -i $input -s $s -a $a -c $c -x $x -A $A -B $B
for i in {1..22} {X,XY,Y,MT};
	do mv chr_$i $RUNDIR/opticall_input;
done
```

3. Use optiCall to call genotypes 
-------------------------------------------------------------------------
In order to call chromosomes seperately, which saves a lot of time, run this script seperately. 
Make sure the [info](https://opticall.bitbucket.io/#info-option-desc) file for opticall is in your $RUNDIR/opticall_input directory. 
MANUALLY adjust the script "$RUNDIR/set_opticall_variables.sh to the appropriate input variables ($RUNDIR and $info).

```
RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]
cd $RUNDIR
mkdir $RUNDIR/opticall_output
bash set_opticall_variables.sh
for i in {1..22} {X,Y,MT};
	do
	sbatch scripts/chr_"$i"_opticall.sh;
	done
```


4. Convert optiCall output into plink binary files
-------------------------------------------------------------------------
Mannualy set your $RUNDIR and $sexinfo
Make sure your [sex.info] file is stored in your $RUNDIR

```
#!/bin/bash
#SBATCH --job-name=GSA.2
#SBATCH --output=GSA.2.out
#SBATCH --error=GSA.2.err
#SBATCH --time=10:00:00
#SBATCH --cpus-per-task=1
#SBATCH --mem=10gb
#SBATCH --nodes=1
#SBATCH --get-user-env=L

module load plink
module load Python

RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]
sexinfo=$RUNDIR/[your_sex.info]

cd $RUNDIR
mkdir $RUNDIR/plink_files
rm $RUNDIR/scripts/chr_*_opticall.sh
rm $RUNDIR/opticall_input/chr_*
rm $RUNDIR/slurm-*

#Convert optiCall output into transposed plink files
for i in {1..22} {X,Y}; 
	do python scripts/OptiCall_to_plink.py opticall_output/chr_"$i".calls plink_files/chr_"$i".tped plink_files/chr_"$i".tfam "$i";
	done
	
# Convert transposed plink files into binary plink files. To avoid heterozygous haploid warnings that involve the X chromosome we use --set-hh-missing, since we are sure at this stage there are no gender errors in our .fam file yet.
for i in {1..22} {X,Y};
	do plink --tfile plink_files/chr_"$i" --recode --make-bed --set-hh-missing --out plink_files/chr_"$i" --allow-no-sex;
	done
	
# Remove large temporary files
for i in {1..22} {X,Y,MT}; 
	do rm plink_files/chr_"$i".tped plink_files/chr_"$i".tfam opticall_output/chr_"$i".calls opticall_output/chr_"$i".probs;
	done
	
# Merge all chromosomes 1-22, X and Y
cat plink_files/chr_X.fam > plink_files/GSApreQC.fam
cat plink_files/chr_{{1..22},X,Y}.bim > plink_files/GSApreQC.bim
(echo -en "\x6C\x1B\x01"; tail -qc +4 plink_files/chr{{1..22},X,Y}.bed) > plink_files/GSApreQC.bed

# Add sex to .fam file. Your [sex.info] file should be stored in your $RUNDIR
plink --bfile plink_files/GSApreQC --update-sex $sexinfo --make-bed --out plink_files/GSApreQC 

# Make directory for pca
mkdir $RUNDER/pca


5. We make use of the Ricopili pipeline to run pre imputation QC and PCA on the Broad Cluster
---------------------------------------------------------------------------------------------

Transfer the files from the HPC cluster to Broad cluster
```
RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]
for i in {bim,bed,fam};
	do scp lobby+calculon:$RUNDIR/plink_files/GSApreQC."$i" mvoskuil@login:/home/unix/mvoskuil; 
	done
```

6. Configurate Ricopili on Broad Cluster
----------------------------------------

``` 
ssh mvoskuil@login

```

First you have to install Ricopili on the Broad cluster. To do so, follow the detailed manual at [Ricopili installation](https://sites.google.com/a/broadinstitute.org/ricopili/installation) 
For now consider Ricopili installed in mvoskuil@login:/home/unix/mvoskuil/rp_bin
To run Ricopili you have to install PDF latex (TexLive). Unfortunately, this is too big to install in your home directory at the Broad cluster, so you have to install it in the hptmp folder. However, files in this folder will be deleted every 14 days.  
Follow the instruction on the TexLive website: [TexLive](https://www.tug.org/texlive/) 

```
date=[current_date_yyyymmdd]
RUNDIR=/broad/hptmp/mvoskuil/[your_RUNDIR]
mkdir $RUNDIR/texlive_$date
cd $RUNDIR/texlive_$date
wget http://mirror.ctan.org/systems/texlive/tlnet/install-tl-unx.tar.gz
tar xvzf install-tl-unx.tar.gz
cd install-tl-$date/
./install-tl
# Enter command:
D
# Enter command:
1
# Enter path:
$RUNDIR/texlive_$date
# Enter command:
R
# Enter command:
I
# TexLive will now start the installation; depending on the connection this may over two hours


```
After installed, add it to your path:
```
#Example
PATH=$RUNDIR/texlive_$date/bin/x86_64-linux:$PATH

```

Every time you login to the Broad cluster, configurate the pipeline:

```
# Use UGER job scheduler 
use UGER

# Add PDF Latex to your PATH
PATH=$RUNDIR/texlive_$date/bin/x86_64-linux:$PATH

# Configurate Ricopili
/home/unix/mvoskuil/rp_bin/rp_config
```

7. Perform QC with Ricopili 
---------------------------

Make sure you have configured Ricopili according to step 6.
Follow instructions at [Ricopili preimputation-QC](https://sites.google.com/a/broadinstitute.org/ricopili/preimputation-qc)

```
# Set run directory
RUNDIR=/broad/hptmp/mvoskuil/[your_RUNDIR]

# Set name of your file
name=[name_your_project i.e. 'GSA']

# Make output directory
mkdir $RUNDIR/GSA_rp_out

# Copy your BINARY plink files into this directory
cp /home/unix/mvoskuil/GSApreQC.bed $RUNDIR/$name_rp_out
cp /home/unix/mvoskuil/GSApreQC.bim $RUNDIR/$name_rp_out
cp /home/unix/mvoskuil/GSApreQC.fam $RUNDIR/$name_rp_out

#Run the following command to run the QC:
cd $RUNDIR/$name_rp_out
preimp --dis ibd --pop mix --out $name
```

When prompted, edit the text file ending in $name.names using a text editor such as emacs, vim or nano
The number of lines in this file corresponds to the number of datasets in the working directory. Each line will have two columns where the second column is the root name of one PLINK file. 
Modify the first column to a 4 letter identifier for the file (ex: location data was obtained from. In here I use: 'PSIc').

```
nano ibd.names
```


```
# Re-run the following command to run the QC
preimp	--dis ibd --pop mix --out $name
```


7b. Perform QC with Ricopili: output
-----------------------------------

Look at the output files in the qc/ directory
PLINK files with -qc extension will be in your directory for each file in disease.names.
The naming of the files is disease_batch_popname_initials-qc.[bed,bim,fam], where disease is the 3 letter phenotype abbreviation specified by --dis, batch is the cohort identifier in disease.names, popname is the population name specified by --popname, and initials are the user's 2 letter initials specified in $HOME/ricopili.conf.
The file qc/disease_batch_popname_initials-qc.pdf.gz contains a summary of the qc that occurred for each batch including the parameters used.
For a more detailed description of the output files, see here: [Ricopili preimp Output-Files](https://sites.google.com/a/broadinstitute.org/ricopili/preimputation-qc#TOC-Output-Files)

8. Perform pre-imputation PCA to check for population stratification
--------------------------------------------------------------------

To assess relatedness and population stratification, the Ricopili pipeline only uses SNP passing the following filters:

	- SNPs are found in all datasets
	- MAF > 5%
	- HWE > 1.0e-04
	- MISSING RATE < 2%
	- no AT/GC SNPs (Strand Ambiguous SNPs)
	- no MHC (6:25-35Mb)
	- no Chr.8 inversion (8:7-13Mb)

Ricopili then prunes SNPs to ensure that there was little linkage disequilibrium between SNPs (R2 < 0.2).

	- LD - R2 < .2, 200 SNPs window: plink --indep-pairwise 200 100 0.2
	- repeat LD pruning with resulting LD pruned dataset
	- if still over 100K SNPs (rare) prune randomly

The resulting SNPs are used to assess common ancestry and population with Eigenstrat


```
# Make directory for your output:
mkdir $RUNDIR/pca_$name
cd $RUNDIR/pca_$name

# Copy (or link) your QC'ed files into this directory:
ln -s $RUNDIR/rp_out_$name/ibd_PSIc_mix_mv-qc.* $RUNDIR/pca_$name

# Run the following command to run the PCA script
pcaer --out $name-[ref] ibd_PSIc_mix_mv-qc.bim # optional: [bfile_ref_populations].bim 

# Example for the Amsterdam (amca) cohort:

# Make sure you also have put the corresponding .fam and .bed files in the directory

# pcaer --out amca-4pop-test ibd_amca_mix_mv-qc.bim pop_4pop_mix_SEQ.bim
```


8b. Perform pre-imputation PCA to check for population stratification: output
-----------------------------------------------------------------------------

Ricopili will output many files: a full decription is available here: [Ricopili PCA Output-Files](https://sites.google.com/a/broadinstitute.org/ricopili/pca#TOC-Output-Files)

We will look at the following files, since we have done the PCA per center;

	- PSIc-[ref].menv.mds.2ds.pdf
	

Visually inspect the plots and extract populations of interest.

```
# Example
awk '{ if ($4 <= -0.019) print $1,$2 }' PSIc-[ref].menv.mds_cov > 1kg_european_samples.txt
awk '{ if ($4 >= -0.0189 && $4 <= 0.005) print $1,$2 }' PSIc-[ref].menv.mds_cov > 1kg_admixed_samples.txt
```
Copy the files back to your local machine
```
scp mvoskuil@login:$RUNDIR/1kg_european_samples.txt /Users/michielvoskuil/Documents/Werk/Promotie/GSA/redo
scp mvoskuil@login:$RUNDIR/1kg_admixed_samples.txt /Users/michielvoskuil/Documents/Werk/Promotie/GSA/redo
```

Now, first we have to remove the 1000G IDs from the text files. Do this with whatever program you like. Also possible with Excel. 

Once removed the 1000G IDs from the .txt files, I'm left with files like this with contain GSA samples per ethnicity, per center:
```
# Example
GSA_european_samples.txt
GSA_admixed_samples.txt
```

Now we copy these files back to the HPC cluster

```
scp /Users/michielvoskuil/Documents/Werk/Promotie/GSA/redo/GSA_european_samples.txt lobby+calculon:$RUNDER/pca
scp /Users/michielvoskuil/Documents/Werk/Promotie/GSA/redo/GSA_admixed_samples.txt lobby+calculon:$RUNDER/pca

scp -3 mvoskuil@login:$RUNDIR/rp_out_$name/ibd_PSIc_mix_mv-qc.bim lobby+calculon:$RUNDER/pca
scp -3 mvoskuil@login:$RUNDIR/rp_out_$name/ibd_PSIc_mix_mv-qc.bed lobby+calculon:$RUNDER/pca
scp -3 mvoskuil@login:$RUNDIR/rp_out_$name/ibd_PSIc_mix_mv-qc.fam lobby+calculon:$RUNDER/pca
```


9. Pre-imputation checking
--------------------------

We will 'clean' our data prior to imputation. All credits for this step go to Will Rayner: [#Checking](http://www.well.ox.ac.uk/~wrayner/tools/#Checking)

From this step onwards we will only work on the HPC cluster.
Make sure your have the script HRC-1000G-check-bim.pl in your $RUNDIR/scripts directory

```
module load plink

# Set your RUNDIR and QC'ed file
RUNDIR=/groups/umcg-weersma/tmp04/[your_RUNDIR]
postQC=[your_post_qc_file]
cd $RUNDIR/pca

# Extract samples for QC'ed files based on ethnicity
plink --bfile $RUNDIR/pca/$postQC --keep $RUNDIR/pca/1kg_european_samples.txt --make-bed --out GSA-european
plink --bfile $RUNDIR/pca/$postQC --keep $RUNDIR/pca/1kg_admixed_samples.txt --make-bed --out GSA-admixed

# Create imputation directory
mkdir $RUNDIR/imputation

# Compile frequency file necessary for the preimputation checking script
ln -s $RUNDIR/pca/GSA-european.* $RUNDIR/imputation
ln -s $RUNDIR/pca/GSA-admixed.* $RUNDIR/imputation
cd $RUNDIR/imputation
plink --bfile GSA-european --freq --out GSA-european
plink --bfile GSA-admixed --freq --out GSA-admixed

# Download HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz from http://www.haplotype-reference-consortium.org/site 
# (Filesize ~640MB zipped and ~ 2.6GB unzipped)
wget ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz
gzip -d HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz 

# Download script "HRC-1000G-check-bim.pl" developed by Will Rayner available at http://www.well.ox.ac.uk/~wrayner/tools/#Checking 
wget http://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.7.zip
unzip HRC-1000G-check-bim-v4.2.7.zip

# Script requires approximately 20GB memory to run. Suggest to run with scheduler.



```












rm -r $RUNDIR/opticall_input etc.. 

UNTIL HERE




Download  HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz from http://www.haplotype-reference-consortium.org/site 
(Filesize ~640MB zipped and ~ 2.6GB unzipped)
```
wget ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz
gzip -d HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz 
```

Download script "HRC-1000G-check-bim.pl" ('Tools') developed by Will Rayner at http://www.well.ox.ac.uk/~wrayner/tools/#Checking 
```
wget http://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.7.zip
unzip HRC-1000G-check-bim-v4.2.7.zip
```

Script requires approximately 20GB memory to run. Suggest to run with scheduler.

```
# Usage: perl HRC-1000G-check-bim.pl -b <bim file> -f <Frequency file> -r <Reference panel> -g -p <population>

# Below I have prepared a script which will create jobs. Just modify your input file names in the script "create_HRC_jobs.sh" ('Tools').

bash create_HRC_jobs.sh


for i in {european,admixed}; do
sbatch HRC-1000G-check-bim."$i".sh;
done 
```

9b. Pre-imputation checking: output
--------------------------

Depending on the size of your data, this script takes approximately 8 minutes to run.
The output will be a ready-to-use script "Run-plink.sh" to update your .bim files.

Running this script will modify your .bim file according to the desired format for HRC imputation

```
bash Run-plink.sh
```

This will create updated post qc binary plink files per CHR (i.e. all_european-qc-updated-chr1)

We now need to remove the underscores in the sample IDs (.fam file), since plink merges FID and IID with underscore when converting to VCF

```
cd /groups/umcg-weersma/tmp04/Michiel/GSA/imputation/check_files_HRC
for i in {european,admixed};
do
	for x in {1..23};
	do
	sed 's/_/-/g' all_"$i"-qc-updated-chr"$x".fam > all_"$i"-qc-updated-chr"$x"-pre-vcf.fam
	done
done

cd /groups/umcg-weersma/tmp04/Michiel/GSA/imputation/check_files_HRC
for i in {european,admixed};
do
	for x in {1..23};
	do
	cp all_"$i"-qc-updated-chr"$x"-pre-vcf.fam ../imputation_ready
	cp all_"$i"-qc-updated-chr"$x".bim ../imputation_ready/all_"$i"-qc-updated-chr"$x"-pre-vcf.bim
	cp all_"$i"-qc-updated-chr"$x".bed ../imputation_ready/all_"$i"-qc-updated-chr"$x"-pre-vcf.bed
	done
done
```

Now we need to convert this to VCF prior to the upload to the Michigan Imputation Server

```
cd /groups/umcg-weersma/tmp04/Michiel/GSA/imputation/imputation_ready

# European
for i in {1..23};
do
module load plink
plink --bfile all_european-qc-updated-chr"$i"-pre-vcf --recode vcf --out all_european-qc-updated-chr"$i"
rm all_european-qc-updated-chr"$i"-pre-vcf.*;
done

# Admixed
for i in {1..23};
do
module load plink
plink --bfile all_admixed-qc-updated-chr"$i"-pre-vcf --recode vcf --out all_admixed-qc-updated-chr"$i"
rm all_admixed-qc-updated-chr"$i"-pre-vcf.*;
done
```

Before we upload to the Michigan imputation server, we have to sort and zip the vcf files:
```
# This step takes approximately 5 minutes

module load VCFtools
for i in {european,admixed};
do
for x in {1..23};
do
vcf-sort all_"$i"-qc-updated-chr"$x".vcf | bgzip -c > all_"$i"-qc-updated-chr"$x".vcf.gz;
done;
done
```

Copy imputation ready files to HD as back-up
```
for i in {european,admixed};
do
	for x in {1..23};
	do
	scp lobby+calculon:/groups/umcg-weersma/tmp04/Michiel/GSA/imputation/imputation_ready/all_"$i"-qc-updated-chr"$x".vcf.gz /Volumes/Michiel/IDAT;
	done;
done
```


10. Upload files to Michigan imputation server and run imputation.
----------------------------------------------------------------------------------------------------

Follow the detailed instructions on their website: [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html)
Make sure you download your results within a week, or your results will be deleted from their server.

11. Visualise and check imputation results. 
-------------------------------------------

We will now check the imputation results, also via a script developed by Will Rayner: [#IC](http://www.well.ox.ac.uk/~wrayner/tools/Post-Imputation.html)

First unzip all results with the password sent to you by email from Michigan server:
```
for i in {1..22}; do
	unzip chr_"$i".zip;
	done
```

The current version requires only the first 8 columns from the VCF output file, use the "vcfparse.pl" script ('Tools') to extract them.


```
# This takes 6-7 hours per chromosome, so best do submit this as a job to the scheduler. 
# The script takes all VCFs in a folder, so best to create separate folders per input (per CHR). 

for i in {1..22}; do
	mkdir ./chr"$i"
	mv chr"$i".dose.vcf.gz ./chr"$i";
	done
```

Usage:
```
# ./vcfparse.pl -d <directory of VCFs> -o <outputname>  [-g]
# - d The path to the directory containing imputed VCF files.
# -o Specifies the output directory name, will be created if it doesn't exist.
# -g Flag to specify the output files are gzipped.
# The program will not overwrite files of the same name and this process will be required for each imputed data set.
```

Create jobs:
```
bash create_cut_jobs.sh
```

Submit jobs to scheduler:
```
for i in {1..22}; do
	sbatch cut_eur_chr"$i".sh;
	done
```

The output files will look like chr"$i".dose.vcf.cut.gz.
With these 'cutted' vcf files we can do the actual post imputation check. We make use of a script developped by Will Rayner ic.pl ('Tools'). The script requires a few dependencies, please see [a link](http://www.well.ox.ac.uk/~wrayner/tools/Post-Imputation.html)


Usage:
```
ic -d <directory> -r <Reference panel> [-h | (-g -p <population> )]  [-f <mappings file>] [-o <output directory>]

# Options:
# -d 			Top level directory containing either one set of per chromosome files, or multiple directories each containing a set of per chromosome files
# 				This directory will be searched recursively for files matching the required formats
# 				Files may be gzipped or uncompressed
# -f --file		Mapping file of directory name to cohort name, optional but recommended when using multiple data sets
# -r --ref		Reference panel	Reference panel summary file, either 1000G or the tab delimited HRC (r1 or r1.1)
# -h --hrc		Flag to indicate Reference panel file is HRC, defaults to HRC if no option is given
# -g --1000g 	Flag to indicate Reference panel file is 1000G
# -p --pop		Population to check allele frequency against
# 				Applies to 1000G only, defaults to ALL if not supplied. 
#				Options available ALL, EUR, AFR, AMR, SAS, EAS
# -o 			Output Directory: Top level directory to contain all the output folders

#Mapping file

# The mapping file should consist of two columns:
# The directory name (optionally including the path)
# The name you wish to use for the output files

# Example mapping file:
# /full/path/to/folder/1	MyStudy1
# ./path/to/folder/2		MyStudy2
# folder3                 	MyStudy3



If no mapping file is supplied the program will attempt to determine a unique set of names from the top level directory and/or sub-directories supplied with the -d option, this may or may not end up with unique folders for each output, if not the program will start an auto-increment on the file names within the directory (these will be consistent across each data set).
One advantage of using a mapping file is the data sets provided need not be all in the same base path.
```

Create jobs:

```
bash create_IC_jobs.sh
```

Submit jobs to scheduler:

```
sbatch IC_admixed_to_HRC_AMR.sh
sbatch IC_admixed_to_HRC_EUR.sh
sbatch IC_european_to_HRC_EUR.sh
```

11. Visualise and check imputation results: output
-------------------------------------------------


These jobs will output many files, most interesting where all results are summarized is the summaryOutput/STUDY.html 

